{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OCT Image Classification - Explainable AI (XAI)\n",
        "\n",
        "## Overview\n",
        "This notebook provides explainability for the OCT classification model using multiple XAI techniques:\n",
        "- **Grad-CAM**: Gradient-weighted Class Activation Mapping\n",
        "- **LIME**: Local Interpretable Model-agnostic Explanations\n",
        "- **Integrated Gradients**: Attribution scores showing pixel importance\n",
        "\n",
        "## Purpose\n",
        "Make the deep learning model's decisions interpretable by:\n",
        "1. Highlighting which regions of the retinal image influenced the prediction\n",
        "2. Providing quantitative metrics about model focus\n",
        "3. Comparing different explanation methods\n",
        "4. Building trust in model predictions for clinical use\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install XAI requirements (uncomment if needed)\n",
        "# !pip install -r requirements_xai.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import XAI utilities\n",
        "from xai_utils import (\n",
        "    preprocess_image_for_xai,\n",
        "    generate_gradcam,\n",
        "    generate_integrated_gradients,\n",
        "    generate_lime_explanation,\n",
        "    apply_colormap_on_image,\n",
        "    create_comparison_plot,\n",
        "    calculate_explanation_metrics,\n",
        "    visualize_with_bounding_boxes,\n",
        "    CLASS_NAMES\n",
        ")\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'model_path': 'classification_models/best_oct_classifier.pth',\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'num_classes': 4,\n",
        "    'img_size': 224,\n",
        "    'output_dir': 'xai_explanations'\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\nClass names: {CLASS_NAMES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Pre-trained Classification Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_classification_model(num_classes=4, pretrained=False):\n",
        "    \"\"\"Create ResNet50 classifier (same architecture as training)\"\"\"\n",
        "    model = models.resnet50(pretrained=pretrained)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(num_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model from: {CONFIG['model_path']}\")\n",
        "model = create_classification_model(num_classes=CONFIG['num_classes'])\n",
        "\n",
        "if os.path.exists(CONFIG['model_path']):\n",
        "    checkpoint = torch.load(CONFIG['model_path'], map_location=CONFIG['device'])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(CONFIG['device'])\n",
        "    model.eval()\n",
        "    print(\"✓ Model loaded successfully!\")\n",
        "    print(f\"  Validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
        "else:\n",
        "    print(f\"❌ Model not found at {CONFIG['model_path']}\")\n",
        "    print(\"Please train the classification model first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Single Image Explanation Function\n",
        "\n",
        "The main function that generates all XAI explanations for a single image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def explain_prediction(image_path, save_results=True):\n",
        "    \"\"\"\n",
        "    Generate all XAI explanations for a single image\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to input image\n",
        "        save_results: Whether to save results to disk\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing all explanations and metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Explaining prediction for: {os.path.basename(image_path)}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Preprocess image\n",
        "    image_tensor, original_image = preprocess_image_for_xai(image_path)\n",
        "    image_tensor = image_tensor.to(CONFIG['device'])\n",
        "    \n",
        "    # 1. Generate Grad-CAM\n",
        "    print(\"\\n[1/3] Generating Grad-CAM...\")\n",
        "    gradcam_heatmap, pred_class, confidence, class_probs = generate_gradcam(\n",
        "        model, image_tensor\n",
        "    )\n",
        "    \n",
        "    # Resize original for overlay\n",
        "    original_resized = cv2.resize(original_image, (224, 224))\n",
        "    gradcam_overlay = apply_colormap_on_image(original_resized, gradcam_heatmap, alpha=0.4)\n",
        "    \n",
        "    print(f\"  ✓ Predicted: {CLASS_NAMES[pred_class]} (Confidence: {confidence*100:.1f}%)\")\n",
        "    \n",
        "    # 2. Generate Integrated Gradients\n",
        "    print(\"\\n[2/3] Generating Integrated Gradients...\")\n",
        "    ig_map, _, _, _ = generate_integrated_gradients(model, image_tensor, n_steps=50)\n",
        "    ig_overlay = apply_colormap_on_image(original_resized, ig_map, alpha=0.4)\n",
        "    print(\"  ✓ Done\")\n",
        "    \n",
        "    # 3. Generate LIME Explanation\n",
        "    print(\"\\n[3/3] Generating LIME explanation (this may take a minute)...\")\n",
        "    lime_image, lime_mask, _, _, _ = generate_lime_explanation(\n",
        "        model, image_tensor, original_image, num_samples=500, num_features=5\n",
        "    )\n",
        "    print(\"  ✓ Done\")\n",
        "    \n",
        "    # Calculate metrics\n",
        "    print(\"\\nCalculating explanation metrics...\")\n",
        "    gradcam_metrics = calculate_explanation_metrics(gradcam_heatmap)\n",
        "    ig_metrics = calculate_explanation_metrics(ig_map)\n",
        "    \n",
        "    # Create bounding box visualization\n",
        "    gradcam_bbox = visualize_with_bounding_boxes(original_resized, gradcam_heatmap, num_regions=5)\n",
        "    \n",
        "    # Create comparison plot\n",
        "    print(\"\\nCreating comparison visualization...\")\n",
        "    fig = create_comparison_plot(\n",
        "        original_resized, gradcam_overlay, lime_image, ig_overlay,\n",
        "        pred_class, confidence, class_probs\n",
        "    )\n",
        "    \n",
        "    # Save results if requested\n",
        "    if save_results:\n",
        "        base_name = Path(image_path).stem\n",
        "        output_prefix = os.path.join(CONFIG['output_dir'], f\"{base_name}_{CLASS_NAMES[pred_class]}\")\n",
        "        \n",
        "        # Save comparison plot\n",
        "        comparison_path = f\"{output_prefix}_comparison.png\"\n",
        "        fig.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
        "        \n",
        "        # Save individual visualizations\n",
        "        cv2.imwrite(f\"{output_prefix}_gradcam.png\", cv2.cvtColor(gradcam_overlay, cv2.COLOR_RGB2BGR))\n",
        "        cv2.imwrite(f\"{output_prefix}_lime.png\", cv2.cvtColor(lime_image, cv2.COLOR_RGB2BGR))\n",
        "        cv2.imwrite(f\"{output_prefix}_ig.png\", cv2.cvtColor(ig_overlay, cv2.COLOR_RGB2BGR))\n",
        "        cv2.imwrite(f\"{output_prefix}_gradcam_bbox.png\", cv2.cvtColor(gradcam_bbox, cv2.COLOR_RGB2BGR))\n",
        "        \n",
        "        # Save metrics\n",
        "        metrics_dict = {\n",
        "            'image_path': image_path,\n",
        "            'predicted_class': CLASS_NAMES[pred_class],\n",
        "            'confidence': float(confidence),\n",
        "            'class_probabilities': {CLASS_NAMES[i]: float(class_probs[i]) for i in range(len(CLASS_NAMES))},\n",
        "            'gradcam_metrics': gradcam_metrics,\n",
        "            'integrated_gradients_metrics': ig_metrics\n",
        "        }\n",
        "        \n",
        "        metrics_path = f\"{output_prefix}_metrics.json\"\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics_dict, f, indent=2)\n",
        "        \n",
        "        print(f\"\\n✓ Results saved to: {CONFIG['output_dir']}/\")\n",
        "    \n",
        "    # Display results\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed metrics\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"EXPLANATION METRICS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nPrediction: {CLASS_NAMES[pred_class]} (Confidence: {confidence*100:.1f}%)\")\n",
        "    print(f\"\\nClass Probabilities:\")\n",
        "    for i, (name, prob) in enumerate(zip(CLASS_NAMES, class_probs)):\n",
        "        bar = '█' * int(prob * 50)\n",
        "        print(f\"  {name:8s}: {prob*100:5.1f}% {bar}\")\n",
        "    \n",
        "    print(f\"\\nGrad-CAM Metrics:\")\n",
        "    print(f\"  Attribution coverage: {gradcam_metrics['attribution_coverage_percent']:.1f}%\")\n",
        "    print(f\"  Peak location: ({gradcam_metrics['peak_activation_location']['x']}, {gradcam_metrics['peak_activation_location']['y']})\")\n",
        "    print(f\"  Mean attribution: {gradcam_metrics['mean_attribution']:.3f}\")\n",
        "    \n",
        "    print(f\"\\nIntegrated Gradients Metrics:\")\n",
        "    print(f\"  Attribution coverage: {ig_metrics['attribution_coverage_percent']:.1f}%\")\n",
        "    print(f\"  Peak location: ({ig_metrics['peak_activation_location']['x']}, {ig_metrics['peak_activation_location']['y']})\")\n",
        "    print(f\"  Mean attribution: {ig_metrics['mean_attribution']:.3f}\")\n",
        "    \n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    return {\n",
        "        'predicted_class': CLASS_NAMES[pred_class],\n",
        "        'confidence': confidence,\n",
        "        'gradcam_overlay': gradcam_overlay,\n",
        "        'lime_image': lime_image,\n",
        "        'ig_overlay': ig_overlay\n",
        "    }\n",
        "\n",
        "print(\"✓ Explanation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Explain a single image\n",
        "# Update this path to your image\n",
        "image_path = 'uploads/example_image.jpg'  # Change this to your image path\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(image_path):\n",
        "    print(f\"⚠️ Image not found: {image_path}\")\n",
        "    print(\"\\nPlease update the image_path variable to point to a valid OCT image.\")\n",
        "else:\n",
        "    # Generate explanations\n",
        "    results = explain_prediction(image_path, save_results=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interpretation Guide\n",
        "\n",
        "### Understanding the Explanations\n",
        "\n",
        "#### Grad-CAM (Gradient-weighted Class Activation Mapping)\n",
        "- **What it shows**: Regions that most strongly influence the model's prediction\n",
        "- **Colors**: Red = high importance, Blue = low importance\n",
        "- **Interpretation**: Areas highlighted in red are where the model \"looks\" to make its decision\n",
        "- **Clinical use**: Verify the model focuses on pathological features (fluid, deposits, etc.) not artifacts\n",
        "\n",
        "#### LIME (Local Interpretable Model-agnostic Explanations)\n",
        "- **What it shows**: Superpixels (image regions) that contribute to the prediction\n",
        "- **Colors**: Highlighted boundaries show important regions\n",
        "- **Interpretation**: The model's decision is based on these specific image patches\n",
        "- **Clinical use**: Understand which anatomical regions drive the diagnosis\n",
        "\n",
        "#### Integrated Gradients\n",
        "- **What it shows**: Pixel-level attribution showing each pixel's contribution\n",
        "- **Colors**: Red = positive contribution, Blue = negative contribution\n",
        "- **Interpretation**: Quantifies how much each pixel influences the final prediction\n",
        "- **Clinical use**: Fine-grained analysis of which features matter most\n",
        "\n",
        "### Clinical Validation Checklist\n",
        "✓ Does the model focus on relevant anatomical structures?  \n",
        "✓ Are pathological features (fluid, deposits) highlighted?  \n",
        "✓ Is the model ignoring irrelevant artifacts or edges?  \n",
        "✓ Do all three XAI methods show consistent focus areas?  \n",
        "✓ Does the explanation align with clinical reasoning?\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
