{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Y7umpx8_rX",
        "outputId": "8c0efd6a-0ddf-4860-f4b2-2fae19ca98b9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2IqeCxEwwWq",
        "outputId": "a66dd193-2f53-4951-98e1-259a9b124f0a"
      },
      "outputs": [],
      "source": [
        "!pip install -U albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R58rCXpHFma",
        "outputId": "e80fbf0d-f291-4a13-c074-ce2802150127"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Project paths\n",
        "PROJECT_FOLDER_IN_DRIVE = '/content/drive/MyDrive/oct_major_project/'\n",
        "\n",
        "# Define all 4 dataset folders\n",
        "DATASETS = {\n",
        "    'NORMAL': 'NORMAL 2.v1i.coco-segmentation',\n",
        "    'DME': 'DME 2.v1i.coco-segmentation',\n",
        "    'CNV': 'CNV 2.v1i.coco-segmentation',\n",
        "    'DRUSEN': 'drusen 3.v1i.coco-segmentation'  # Using version 3\n",
        "}\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Training hyperparameters\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 75  # Increased from 75 for better convergence with scheduler\n",
        "\n",
        "# ========================================\n",
        "# UNIFIED CATEGORY MAPPING\n",
        "# ========================================\n",
        "\n",
        "# Define unified categories across all datasets\n",
        "UNIFIED_CATEGORIES = {\n",
        "    0: 'background',\n",
        "    1: 'GCL',                   # Ganglion Cell Layer\n",
        "    2: 'INL',                   # Inner Nuclear Layer\n",
        "    3: 'IPL',                   # Inner Plexiform Layer\n",
        "    4: 'ONL',                   # Outer Nuclear Layer\n",
        "    5: 'OPL',                   # Outer Plexiform Layer\n",
        "    6: 'RNFL',                  # Retinal Nerve Fiber Layer\n",
        "    7: 'RPE',                   # Retinal Pigment Epithelium\n",
        "    8: 'CHOROID',               # Choroid (only in DME)\n",
        "    9: 'INTRA-RETINAL-FLUID',   # Fluid within retinal layers\n",
        "    10: 'SUB-RETINAL-FLUID',    # Fluid below retina (CNV)\n",
        "    11: 'PED',                  # Pigment Epithelial Detachment\n",
        "    12: 'DRUSENOID-PED'         # Drusen-specific PED\n",
        "}\n",
        "\n",
        "NUM_CLASSES = len(UNIFIED_CATEGORIES)\n",
        "print(f\"\\nTotal unified classes: {NUM_CLASSES}\")\n",
        "print(\"Categories:\", list(UNIFIED_CATEGORIES.values()))\n",
        "\n",
        "# ========================================\n",
        "# DATASET-SPECIFIC MAPPINGS\n",
        "# ========================================\n",
        "\n",
        "# Map original category IDs to unified IDs for each dataset\n",
        "CATEGORY_MAPPINGS = {\n",
        "    'NORMAL': {\n",
        "        # NORMAL: id -> unified_id\n",
        "        1: 1,   # GCL\n",
        "        2: 2,   # INL\n",
        "        3: 3,   # IPL\n",
        "        4: 4,   # ONL\n",
        "        5: 5,   # OPL\n",
        "        6: 6,   # RNFL\n",
        "        7: 7,   # RPE\n",
        "    },\n",
        "    'DME': {\n",
        "        # DME: id -> unified_id\n",
        "        1: 0,   # \"0\" -> background (artifact, ignore it)\n",
        "        2: 8,   # CHOROID\n",
        "        3: 1,   # GCL\n",
        "        4: 2,   # INL\n",
        "        5: 9,   # INTRA-RETINAL-FLUID\n",
        "        6: 3,   # IPL\n",
        "        7: 4,   # ONL\n",
        "        8: 5,   # OPL\n",
        "        9: 6,   # RNFL\n",
        "        10: 7,  # RPE\n",
        "    },\n",
        "    'CNV': {\n",
        "        # CNV: id -> unified_id\n",
        "        1: 1,   # GCL\n",
        "        2: 2,   # INL\n",
        "        3: 9,   # INTRA-RETINAL-FLUID\n",
        "        4: 3,   # IPL\n",
        "        5: 4,   # ONL\n",
        "        6: 5,   # OPL\n",
        "        7: 11,  # PED\n",
        "        8: 6,   # RNFL\n",
        "        9: 7,   # RPE\n",
        "        10: 10, # SUB-RETINAL-FLUID\n",
        "    },\n",
        "    'DRUSEN': {\n",
        "        # DRUSEN: id -> unified_id\n",
        "        1: 12,  # DRUSENOID-PED\n",
        "        2: 1,   # GCL\n",
        "        3: 2,   # INL\n",
        "        4: 9,   # INTRA-RETINAL-FLUID\n",
        "        5: 3,   # IPL\n",
        "        6: 4,   # ONL\n",
        "        7: 5,   # OPL\n",
        "        8: 11,  # PED\n",
        "        9: 6,   # RNFL\n",
        "        10: 7,  # RPE\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n‚úì Configuration complete!\")\n",
        "print(f\"Datasets to process: {list(DATASETS.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MErr1n1B0qE",
        "outputId": "d2f70d72-0685-42fa-a086-9de28c233628"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# VERIFY CATEGORIES IN ALL DATASETS\n",
        "# ========================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VERIFYING CATEGORIES ACROSS ALL DATASETS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for dataset_name, folder_name in DATASETS.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Build paths\n",
        "    data_dir = os.path.join(PROJECT_FOLDER_IN_DRIVE, folder_name, 'train/')\n",
        "    json_path = os.path.join(data_dir, '_annotations.coco.json')\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(json_path):\n",
        "        print(f\"‚ùå ERROR: JSON file not found at {json_path}\")\n",
        "        continue\n",
        "\n",
        "    # Load COCO annotations\n",
        "    coco = COCO(json_path)\n",
        "\n",
        "    # Get categories\n",
        "    categories = coco.loadCats(coco.getCatIds())\n",
        "\n",
        "    print(f\"\\nFound {len(categories)} categories:\")\n",
        "    print(f\"{'Original ID':<15} {'Category Name':<30} {'‚Üí Unified ID':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Show mapping\n",
        "    mapping = CATEGORY_MAPPINGS[dataset_name]\n",
        "    for cat in categories:\n",
        "        orig_id = cat['id']\n",
        "        cat_name = cat['name']\n",
        "        unified_id = mapping.get(orig_id, 0)\n",
        "        unified_name = UNIFIED_CATEGORIES.get(unified_id, 'UNMAPPED')\n",
        "\n",
        "        print(f\"{orig_id:<15} {cat_name:<30} ‚Üí {unified_id:<3} ({unified_name})\")\n",
        "\n",
        "    # Count images\n",
        "    img_ids = coco.getImgIds()\n",
        "    print(f\"\\n‚úì Total images: {len(img_ids)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"VERIFICATION COMPLETE\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO_VAbnPbL5-",
        "outputId": "f9d38ed4-239d-4bb2-829f-60eeeb5687a5"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# GENERATE UNIFIED MASKS FOR ALL DATASETS\n",
        "# ========================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GENERATING UNIFIED MASKS FOR ALL DATASETS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_images_processed = 0\n",
        "\n",
        "for dataset_name, folder_name in DATASETS.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing: {dataset_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Build paths\n",
        "    data_dir = os.path.join(PROJECT_FOLDER_IN_DRIVE, folder_name, 'train/')\n",
        "    json_path = os.path.join(data_dir, '_annotations.coco.json')\n",
        "    image_dir = data_dir\n",
        "    mask_save_dir = os.path.join(data_dir, 'masks/')\n",
        "\n",
        "    # Create masks directory\n",
        "    os.makedirs(mask_save_dir, exist_ok=True)\n",
        "\n",
        "    # Load COCO annotations\n",
        "    print(f\"Loading COCO annotations from: {json_path}\")\n",
        "    coco = COCO(json_path)\n",
        "\n",
        "    # Get all images\n",
        "    img_ids = coco.getImgIds()\n",
        "    images = coco.loadImgs(img_ids)\n",
        "\n",
        "    print(f\"Found {len(images)} images. Starting mask generation...\")\n",
        "\n",
        "    # Get mapping for this dataset\n",
        "    category_mapping = CATEGORY_MAPPINGS[dataset_name]\n",
        "\n",
        "    # Generate masks\n",
        "    for img_info in tqdm(images, desc=f\"Generating masks for {dataset_name}\"):\n",
        "        img_id = img_info['id']\n",
        "        img_file_name = img_info['file_name']\n",
        "        img_height = img_info['height']\n",
        "        img_width = img_info['width']\n",
        "\n",
        "        # Create blank mask (all background = 0)\n",
        "        mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "        # Get annotations for this image\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Process each annotation\n",
        "        for ann in anns:\n",
        "            category_id = ann['category_id']\n",
        "\n",
        "            # Map to unified category ID\n",
        "            unified_label = category_mapping.get(category_id, 0)\n",
        "\n",
        "            # Draw polygons\n",
        "            for seg in ann['segmentation']:\n",
        "                poly = np.array(seg, dtype=np.int32).reshape((-1, 1, 2))\n",
        "                cv2.fillPoly(mask, [poly], color=int(unified_label))\n",
        "\n",
        "        # Save mask\n",
        "        base_name = img_file_name.split('.')[0]\n",
        "        mask_file_name = f\"{base_name}.png\"\n",
        "        save_path = os.path.join(mask_save_dir, mask_file_name)\n",
        "        cv2.imwrite(save_path, mask)\n",
        "\n",
        "    print(f\"‚úì Completed {dataset_name}: {len(images)} masks saved to {mask_save_dir}\")\n",
        "    total_images_processed += len(images)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"MASK GENERATION COMPLETE!\")\n",
        "print(f\"Total images processed: {total_images_processed}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tLbPhuhoLsC",
        "outputId": "4bce3c1c-c88f-48ac-b130-7ecc4a9f7a6d"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class RetinaDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_file = self.images[index]\n",
        "        mask_file = img_file.split('.')[0] + '.png'\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, img_file)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
        "\n",
        "        # Read image and mask\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Enhanced preprocessing pipeline\n",
        "        # 1. Denoise while preserving edges\n",
        "        image = cv2.fastNlMeansDenoising(image, None, h=10, searchWindowSize=21)\n",
        "\n",
        "        # 2. Enhance edges using unsharp masking\n",
        "        gaussian_3 = cv2.GaussianBlur(image, (0, 0), 2.0)\n",
        "        unsharp_image = cv2.addWeighted(image, 2.0, gaussian_3, -1.0, 0)\n",
        "\n",
        "        # 3. Enhance local contrast\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        image = clahe.apply(unsharp_image)\n",
        "\n",
        "        # 4. Convert grayscale to RGB for albumentations transforms\n",
        "        # (CLAHE and Sharpen require 3-channel images)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask'].long()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=512, width=512, interpolation=cv2.INTER_LANCZOS4),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.OneOf([\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
        "            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0),\n",
        "        ], p=0.3),\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(p=1.0),\n",
        "            A.ISONoise(p=1.0),\n",
        "        ], p=0.2),\n",
        "        A.Sharpen(p=0.3),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=512, width=512, interpolation=cv2.INTER_LANCZOS4),\n",
        "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0),  # Enhance contrast\n",
        "        A.Sharpen(p=1.0),  # Sharpen edges\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Dataset class and augmentation transforms have been updated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjhwlLiFPo54",
        "outputId": "033da361-3a87-4e2e-8680-9d18d9925c61"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# U-NET MODEL ARCHITECTURE\n",
        "# ========================================\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=13, features=[64, 128, 256, 512]):\n",
        "        super(UNet, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Encoder\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Decoder\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        # Encoder\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Reverse skip connections\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        # Decoder\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = torch.nn.functional.interpolate(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "\n",
        "print(\"‚úì U-Net model architecture defined!\")\n",
        "print(f\"Model will output {NUM_CLASSES} classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9_DCsSFFInz",
        "outputId": "dd405bc2-2c60-43a9-9536-2e9cb2133fde"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        # Apply softmax to get probabilities\n",
        "        inputs = torch.softmax(inputs, dim=1)\n",
        "\n",
        "        # Flatten label and prediction tensors\n",
        "        inputs = inputs.reshape(-1)\n",
        "        targets = targets.reshape(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "class EdgeAwareLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, beta=0.25, weight=None):\n",
        "        super(EdgeAwareLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(weight=weight)\n",
        "        self.dice = DiceLoss()\n",
        "        self.sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        self.sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "\n",
        "    def edge_loss(self, pred, target):\n",
        "        # Convert predictions to probabilities\n",
        "        pred_soft = torch.softmax(pred, dim=1)\n",
        "\n",
        "        # Calculate edges for both prediction and target\n",
        "        pred_edges = torch.sqrt(\n",
        "            torch.conv2d(pred_soft[:, 1:].sum(1, keepdim=True), self.sobel_x.to(pred.device), padding=1)**2 +\n",
        "            torch.conv2d(pred_soft[:, 1:].sum(1, keepdim=True), self.sobel_y.to(pred.device), padding=1)**2\n",
        "        )\n",
        "\n",
        "        target_edges = torch.sqrt(\n",
        "            torch.conv2d(target.float(), self.sobel_x.to(target.device), padding=1)**2 +\n",
        "            torch.conv2d(target.float(), self.sobel_y.to(target.device), padding=1)**2\n",
        "        )\n",
        "\n",
        "        return torch.mean(torch.abs(pred_edges - target_edges))\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Convert targets for different loss components\n",
        "        target_one_hot = torch.nn.functional.one_hot(targets, num_classes=NUM_CLASSES).permute(0, 3, 1, 2)\n",
        "\n",
        "        # Calculate individual losses\n",
        "        loss_ce = self.cross_entropy(inputs, targets)\n",
        "        loss_dice = self.dice(inputs, target_one_hot)\n",
        "        loss_edge = self.edge_loss(inputs, targets)\n",
        "\n",
        "        # Combine losses with weights\n",
        "        total_loss = (self.alpha * loss_ce +\n",
        "                     (1 - self.alpha) * loss_dice +\n",
        "                     self.beta * loss_edge)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "print(\"Advanced CombinedLoss (Cross-Entropy + Dice) function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CLASS WEIGHT CALCULATION FUNCTION\n",
        "# ========================================\n",
        "\n",
        "def calculate_class_weights(loader, num_classes, device):\n",
        "    \"\"\"\n",
        "    Calculate class weights based on pixel frequency to handle class imbalance\n",
        "\n",
        "    Args:\n",
        "        loader: DataLoader containing the training data\n",
        "        num_classes: Number of classes in the dataset\n",
        "        device: torch device (cuda/cpu)\n",
        "\n",
        "    Returns:\n",
        "        Tensor of class weights\n",
        "    \"\"\"\n",
        "    class_counts = torch.zeros(num_classes)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CALCULATING CLASS WEIGHTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for batch_idx, (_, targets) in enumerate(tqdm(loader, desc=\"Counting pixels\")):\n",
        "        for c in range(num_classes):\n",
        "            class_counts[c] += (targets == c).sum().item()\n",
        "\n",
        "    # Inverse frequency weighting\n",
        "    total_pixels = class_counts.sum()\n",
        "    class_weights = total_pixels / (num_classes * class_counts + 1e-6)\n",
        "\n",
        "    # Normalize so minimum weight = 1.0\n",
        "    class_weights = class_weights / class_weights.min()\n",
        "\n",
        "    print(f\"\\nClass Distribution and Weights:\")\n",
        "    print(f\"{'Class ID':<10} {'Name':<25} {'Pixel Count':<15} {'Weight':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "    for c in range(num_classes):\n",
        "        if class_counts[c] > 0:\n",
        "            print(f\"{c:<10} {UNIFIED_CATEGORIES[c]:<25} {int(class_counts[c]):,<15} {class_weights[c]:.3f}\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    return class_weights.to(device)\n",
        "\n",
        "print(\"‚úì Class weight calculation function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# DIAGNOSTIC: CHECK WHAT FILES EXIST\n",
        "# ========================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DIAGNOSTIC: CHECKING FILE STRUCTURE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for dataset_name, folder_name in DATASETS.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Build paths\n",
        "    image_dir = os.path.join(PROJECT_FOLDER_IN_DRIVE, folder_name, 'train')\n",
        "    mask_dir = os.path.join(PROJECT_FOLDER_IN_DRIVE, folder_name, 'train', 'masks')\n",
        "\n",
        "    print(f\"Image directory: {image_dir}\")\n",
        "    print(f\"Exists: {os.path.exists(image_dir)}\")\n",
        "\n",
        "    if os.path.exists(image_dir):\n",
        "        all_files = os.listdir(image_dir)\n",
        "        print(f\"Total items in directory: {len(all_files)}\")\n",
        "\n",
        "        # Check for image files\n",
        "        jpg_files = [f for f in all_files if f.endswith('.jpg') and os.path.isfile(os.path.join(image_dir, f))]\n",
        "        png_files = [f for f in all_files if f.endswith('.png') and os.path.isfile(os.path.join(image_dir, f))]\n",
        "        jpeg_files = [f for f in all_files if f.endswith('.jpeg') and os.path.isfile(os.path.join(image_dir, f))]\n",
        "\n",
        "        print(f\"  - .jpg files: {len(jpg_files)}\")\n",
        "        print(f\"  - .png files: {len(png_files)}\")\n",
        "        print(f\"  - .jpeg files: {len(jpeg_files)}\")\n",
        "\n",
        "        # Show first few files\n",
        "        image_files = jpg_files + png_files + jpeg_files\n",
        "        if len(image_files) > 0:\n",
        "            print(f\"  - Sample files: {image_files[:3]}\")\n",
        "        else:\n",
        "            print(f\"  - ‚ö†Ô∏è  NO IMAGE FILES FOUND!\")\n",
        "            print(f\"  - All items in directory: {all_files[:10]}\")\n",
        "\n",
        "    print(f\"\\nMask directory: {mask_dir}\")\n",
        "    print(f\"Exists: {os.path.exists(mask_dir)}\")\n",
        "\n",
        "    if os.path.exists(mask_dir):\n",
        "        mask_files = [f for f in os.listdir(mask_dir) if f.endswith('.png')]\n",
        "        print(f\"  - Mask files: {len(mask_files)}\")\n",
        "        if len(mask_files) > 0:\n",
        "            print(f\"  - Sample masks: {mask_files[:3]}\")\n",
        "        else:\n",
        "            print(f\"  - ‚ö†Ô∏è  NO MASK FILES FOUND!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DIAGNOSTIC COMPLETE\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# COMBINED DATASET CLASS (ROBUST VERSION)\n",
        "# ========================================\n",
        "\n",
        "class CombinedOCTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that combines multiple OCT disease datasets\n",
        "    Includes 'Smart Search' to handle filename mismatches\n",
        "    \"\"\"\n",
        "    def __init__(self, datasets_config, project_folder, transform=None):\n",
        "        self.transform = transform\n",
        "        self.samples = []  # List of (image_path, mask_path, dataset_name) tuples\n",
        "\n",
        "        # Load samples from each dataset\n",
        "        for dataset_name, folder_name in datasets_config.items():\n",
        "            # Try multiple possible image directory locations\n",
        "            possible_image_dirs = [\n",
        "                os.path.join(project_folder, folder_name, 'train'),\n",
        "                os.path.join(project_folder, folder_name, 'train', 'images'),\n",
        "            ]\n",
        "\n",
        "            mask_dir = os.path.join(project_folder, folder_name, 'train', 'masks')\n",
        "\n",
        "            # Find valid image directory\n",
        "            image_dir = None\n",
        "            for possible_dir in possible_image_dirs:\n",
        "                if os.path.exists(possible_dir):\n",
        "                    # Check if directory contains images (not just folders)\n",
        "                    has_images = any(f.lower().endswith(('.jpg', '.png', '.jpeg')) for f in os.listdir(possible_dir))\n",
        "                    if has_images:\n",
        "                        image_dir = possible_dir\n",
        "                        break\n",
        "\n",
        "            if image_dir is None or not os.path.exists(mask_dir):\n",
        "                print(f\"‚ö†Ô∏è  Skipping {dataset_name}: Images or Masks folder missing.\")\n",
        "                continue\n",
        "\n",
        "            # Get list of images\n",
        "            images = sorted([f for f in os.listdir(image_dir)\n",
        "                           if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
        "                           and not f.startswith('.')\n",
        "                           and 'mask' not in f.lower()])\n",
        "\n",
        "            dataset_count = 0\n",
        "\n",
        "            for img_file in images:\n",
        "                img_path = os.path.join(image_dir, img_file)\n",
        "\n",
        "                # STRATEGY 1: Standard Naming (image.01.jpg -> image.01.png)\n",
        "                base_name_strict = img_file.rsplit('.', 1)[0]\n",
        "                mask_file_strict = base_name_strict + '.png'\n",
        "                mask_path_strict = os.path.join(mask_dir, mask_file_strict)\n",
        "\n",
        "                # STRATEGY 2: Simple Naming (image.01.jpg -> image.png)\n",
        "                base_name_simple = img_file.split('.')[0]\n",
        "                mask_file_simple = base_name_simple + '.png'\n",
        "                mask_path_simple = os.path.join(mask_dir, mask_file_simple)\n",
        "\n",
        "                # Check which one exists\n",
        "                if os.path.exists(mask_path_strict):\n",
        "                    self.samples.append((img_path, mask_path_strict, dataset_name))\n",
        "                    dataset_count += 1\n",
        "                elif os.path.exists(mask_path_simple):\n",
        "                    self.samples.append((img_path, mask_path_simple, dataset_name))\n",
        "                    dataset_count += 1\n",
        "\n",
        "            print(f\"  - {dataset_name}: Loaded {dataset_count} pairs (Total images in folder: {len(images)})\")\n",
        "            if dataset_count == 0:\n",
        "                print(f\"    ‚ùå DEBUG: Looked for masks named '{mask_file_strict}' OR '{mask_file_simple}' but found neither.\")\n",
        "\n",
        "        print(f\"‚úì Loaded {len(self.samples)} total image-mask pairs.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, mask_path, dataset_name = self.samples[index]\n",
        "\n",
        "        # Read image and mask\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Safety check for empty files\n",
        "        if image is None:\n",
        "            print(f\"Error loading image: {img_path}\")\n",
        "            # Return a dummy tensor to prevent crash, or raise error\n",
        "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
        "        if mask is None:\n",
        "            print(f\"Error loading mask: {mask_path}\")\n",
        "            raise ValueError(f\"Failed to load mask: {mask_path}\")\n",
        "\n",
        "        # Preprocessing (Same as before)\n",
        "        try:\n",
        "            image = cv2.fastNlMeansDenoising(image, None, h=10, searchWindowSize=21)\n",
        "            gaussian_3 = cv2.GaussianBlur(image, (0, 0), 2.0)\n",
        "            unsharp_image = cv2.addWeighted(image, 2.0, gaussian_3, -1.0, 0)\n",
        "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "            image = clahe.apply(unsharp_image)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "        except Exception as e:\n",
        "            print(f\"Preprocessing error on {img_path}: {e}\")\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB) # Fallback\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask'].long()\n",
        "        else:\n",
        "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
        "            mask = torch.from_numpy(mask).long()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "print(\"‚úì Robust CombinedOCTDataset class updated!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# INITIALIZE MODEL, OPTIMIZER, AND DATALOADERS\n",
        "# ========================================\n",
        "#\n",
        "# ‚ö†Ô∏è  IMPORTANT: Before running this cell, make sure you have:\n",
        "#    1. Run Cell 4 to generate masks from COCO annotations\n",
        "#    2. Verified that masks were created successfully\n",
        "#\n",
        "# If you get an error about 0 images, go back and run Cell 4 first!\n",
        "# ========================================\n",
        "\n",
        "import gc\n",
        "from torch.utils.data import random_split, Subset\n",
        "\n",
        "# GPU Cleanup\n",
        "print(\"Clearing CUDA cache...\")\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Initialize Model\n",
        "print(f\"\\nInitializing U-Net model with {NUM_CLASSES} output classes...\")\n",
        "model = UNet(in_channels=3, out_channels=NUM_CLASSES).to(DEVICE)\n",
        "print(f\"‚úì Model initialized!\")\n",
        "\n",
        "# Initialize Optimizer (will be used after setting up loss function)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "print(f\"‚úì Optimizer initialized with learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Create Combined Dataset\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING COMBINED DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "full_dataset = CombinedOCTDataset(\n",
        "    datasets_config=DATASETS,\n",
        "    project_folder=PROJECT_FOLDER_IN_DRIVE,\n",
        "    transform=None\n",
        ")\n",
        "\n",
        "# Check if dataset is empty\n",
        "if len(full_dataset) == 0:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚ö†Ô∏è  ERROR: NO IMAGES FOUND!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n‚ö†Ô∏è  The dataset is empty. This means masks haven't been generated yet.\")\n",
        "    print(\"\\nüìù TO FIX THIS:\")\n",
        "    print(\"   1. Go back and run Cell 4: 'GENERATE UNIFIED MASKS FOR ALL DATASETS'\")\n",
        "    print(\"   2. Wait for mask generation to complete (this may take a few minutes)\")\n",
        "    print(\"   3. Then come back and run this cell again\")\n",
        "    print(\"\\nüí° Cell 4 will create masks from COCO annotations and save them to:\")\n",
        "    for dataset_name, folder_name in DATASETS.items():\n",
        "        mask_dir = os.path.join(PROJECT_FOLDER_IN_DRIVE, folder_name, 'train', 'masks')\n",
        "        print(f\"   - {mask_dir}\")\n",
        "    print(\"=\"*70)\n",
        "    raise ValueError(\"Dataset is empty. Please run Cell 4 to generate masks first!\")\n",
        "\n",
        "# Split 80/20\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_indices, val_indices = random_split(\n",
        "    range(len(full_dataset)),\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Training: {len(train_indices)} images\")\n",
        "print(f\"‚úì Validation: {len(val_indices)} images\")\n",
        "\n",
        "# Create datasets with transforms\n",
        "train_dataset = CombinedOCTDataset(\n",
        "    datasets_config=DATASETS,\n",
        "    project_folder=PROJECT_FOLDER_IN_DRIVE,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = CombinedOCTDataset(\n",
        "    datasets_config=DATASETS,\n",
        "    project_folder=PROJECT_FOLDER_IN_DRIVE,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "val_subset = Subset(val_dataset, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"‚úì DataLoaders created!\")\n",
        "print(f\"  - Training batches: {len(train_loader)}\")\n",
        "print(f\"  - Validation batches: {len(val_loader)}\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# SETUP LOSS FUNCTION WITH CLASS WEIGHTS\n",
        "# ========================================\n",
        "\n",
        "# Calculate class weights from training data\n",
        "class_weights = calculate_class_weights(train_loader, NUM_CLASSES, DEVICE)\n",
        "\n",
        "# Use EdgeAwareLoss instead of basic CrossEntropyLoss\n",
        "# This combines CE + Dice + Edge loss for better boundary detection\n",
        "loss_fn = EdgeAwareLoss(alpha=0.5, beta=0.25, weight=class_weights)\n",
        "\n",
        "print(f\"\\n‚úì Using EdgeAwareLoss with class weights\")\n",
        "print(f\"  - Alpha (CE weight): 0.5\")\n",
        "print(f\"  - Beta (Edge weight): 0.25\")\n",
        "print(f\"  - Dice weight: 0.5 (1-alpha)\")\n",
        "\n",
        "# ========================================\n",
        "# SETUP LEARNING RATE SCHEDULER\n",
        "# ========================================\n",
        "\n",
        "# ReduceLROnPlateau: Reduces LR when Dice score stops improving\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',              # Maximize Dice score\n",
        "    factor=0.5,              # Reduce LR by 50%\n",
        "    patience=10,             # Wait 10 epochs before reducing\n",
        "    min_lr=1e-7,             # Minimum learning rate\n",
        "    threshold=0.001          # Minimum improvement threshold\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Learning rate scheduler initialized\")\n",
        "print(f\"  - Initial LR: {LEARNING_RATE}\")\n",
        "print(f\"  - Patience: 10 epochs\")\n",
        "print(f\"  - Reduction factor: 0.5x\")\n",
        "print(f\"  - Minimum LR: 1e-7\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J49evVsOuuR",
        "outputId": "5e4791c8-eda7-49ab-99df-679f41a6cab0"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# TRAINING FUNCTION\n",
        "# ========================================\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn, device):\n",
        "    loop = tqdm(loader, desc=\"Training\")\n",
        "    total_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data, targets = data.to(device=device), targets.to(device=device)\n",
        "\n",
        "        predictions = model(data)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# EVALUATION FUNCTION\n",
        "# ========================================\n",
        "\n",
        "def check_metrics(loader, model, device, num_classes):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    iou_score = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(loader, desc=\"Calculating Metrics\"):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            preds = model(x)\n",
        "            preds = torch.argmax(preds, dim=1)\n",
        "\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "\n",
        "            y_one_hot = torch.nn.functional.one_hot(y, num_classes=num_classes).permute(0, 3, 1, 2)\n",
        "            preds_one_hot = torch.nn.functional.one_hot(preds, num_classes=num_classes).permute(0, 3, 1, 2)\n",
        "\n",
        "            intersection = (preds_one_hot * y_one_hot).float().sum()\n",
        "            union = preds_one_hot.float().sum() + y_one_hot.float().sum()\n",
        "\n",
        "            dice_score += (2. * intersection) / (union + 1e-8)\n",
        "            iou_score += intersection / (union - intersection + 1e-8)\n",
        "\n",
        "    pixel_acc = (num_correct / num_pixels) * 100\n",
        "    avg_dice = dice_score / len(loader)\n",
        "    avg_iou = iou_score / len(loader)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Pixel Accuracy: {pixel_acc:.2f}%\")\n",
        "    print(f\"Dice Score: {avg_dice:.4f}\")\n",
        "    print(f\"IoU Score: {avg_iou:.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return pixel_acc, avg_dice, avg_iou\n",
        "\n",
        "\n",
        "print(\"‚úì Training and evaluation functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split, Subset\n",
        "import gc\n",
        "\n",
        "# GPU Cleanup\n",
        "print(\"Clearing CUDA cache...\")\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Initialize Model\n",
        "print(f\"\\nInitializing U-Net model with {NUM_CLASSES} output classes...\")\n",
        "model = UNet(in_channels=3, out_channels=NUM_CLASSES).to(DEVICE)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "print(f\"‚úì Model initialized!\")\n",
        "\n",
        "# Create Combined Dataset\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING COMBINED DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "full_dataset = CombinedOCTDataset(\n",
        "    datasets_config=DATASETS,\n",
        "    project_folder=PROJECT_FOLDER_IN_DRIVE,\n",
        "    transform=None\n",
        ")\n",
        "\n",
        "# Split 80/20\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_indices, val_indices = random_split(\n",
        "    range(len(full_dataset)),\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Training: {len(train_indices)} images\")\n",
        "print(f\"‚úì Validation: {len(val_indices)} images\")\n",
        "\n",
        "# Create datasets with transforms\n",
        "train_dataset = CombinedOCTDataset(\n",
        "    datasets_config=DATASETS,\n",
        "    project_folder=PROJECT_FOLDER_IN_DRIVE,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = CombinedOCTDataset(\n",
        "    datasets_config=DATASETS,\n",
        "    project_folder=PROJECT_FOLDER_IN_DRIVE,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "val_subset = Subset(val_dataset, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"‚úì DataLoaders created!\")\n",
        "\n",
        "# Training Loop\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "training_losses = []\n",
        "validation_accuracies = []\n",
        "validation_dice_scores = []\n",
        "validation_iou_scores = []\n",
        "best_dice = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EPOCH {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    avg_loss = train_fn(train_loader, model, optimizer, loss_fn, DEVICE)\n",
        "    training_losses.append(avg_loss)\n",
        "\n",
        "    print(\"\\nValidation:\")\n",
        "    pixel_acc, dice, iou = check_metrics(val_loader, model, DEVICE, NUM_CLASSES)\n",
        "    validation_accuracies.append(pixel_acc.cpu().numpy() if torch.is_tensor(pixel_acc) else pixel_acc)\n",
        "    validation_dice_scores.append(dice.cpu().numpy() if torch.is_tensor(dice) else dice)\n",
        "    validation_iou_scores.append(iou.cpu().numpy() if torch.is_tensor(iou) else iou)\n",
        "\n",
        "    if dice > best_dice:\n",
        "        best_dice = dice\n",
        "        torch.save(model.state_dict(), os.path.join(PROJECT_FOLDER_IN_DRIVE, \"unet_combined_best.pth\"))\n",
        "        print(f\"‚úì New best model saved! (Dice: {best_dice:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Plot results\n",
        "training_losses = np.array(training_losses)\n",
        "validation_accuracies = np.array(validation_accuracies)\n",
        "validation_dice_scores = np.array(validation_dice_scores)\n",
        "validation_iou_scores = np.array(validation_iou_scores)\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), training_losses, 'b-', linewidth=2)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), validation_accuracies, 'g-', linewidth=2, label='Pixel Accuracy')\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), validation_dice_scores * 100, 'r-', linewidth=2, label='Dice (√ó100)')\n",
        "plt.title('Validation Metrics')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), validation_iou_scores, 'purple', linewidth=2)\n",
        "plt.title('IoU Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('IoU')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(PROJECT_FOLDER_IN_DRIVE, 'training_metrics.png'), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal Loss: {training_losses[-1]:.4f}\")\n",
        "print(f\"Final Accuracy: {validation_accuracies[-1]:.2f}%\")\n",
        "print(f\"Final Dice: {validation_dice_scores[-1]:.4f}\")\n",
        "print(f\"Final IoU: {validation_iou_scores[-1]:.4f}\")\n",
        "print(f\"Best Dice: {best_dice:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.patches as mpatches\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# ========================================\n",
        "# SINGLE IMAGE PREDICTION WITH MANUAL UPLOAD\n",
        "# ========================================\n",
        "\n",
        "def predict_single_image(image_path, model_path, device='cuda'):\n",
        "    \"\"\"Predict segmentation mask for a single image\"\"\"\n",
        "    # Load the model\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    model = UNet(in_channels=3, out_channels=NUM_CLASSES).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(\"‚úì Model loaded successfully!\")\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    print(f\"\\nLoading image: {image_path}\")\n",
        "    original_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if original_image is None:\n",
        "        raise ValueError(f\"Could not load image from {image_path}\")\n",
        "\n",
        "    print(f\"Original image shape: {original_image.shape}\")\n",
        "\n",
        "    # Apply preprocessing\n",
        "    image = cv2.fastNlMeansDenoising(original_image, None, h=10, searchWindowSize=21)\n",
        "    gaussian_3 = cv2.GaussianBlur(image, (0, 0), 2.0)\n",
        "    unsharp_image = cv2.addWeighted(image, 2.0, gaussian_3, -1.0, 0)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    image = clahe.apply(unsharp_image)\n",
        "\n",
        "    # Convert grayscale to RGB for albumentations transforms\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # Apply validation transforms\n",
        "    augmented = val_transform(image=image, mask=np.zeros_like(image))\n",
        "    image_tensor = augmented['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Predict\n",
        "    print(\"Running inference...\")\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_tensor)\n",
        "        predicted_mask = torch.argmax(prediction, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    print(f\"Predicted mask shape: {predicted_mask.shape}\")\n",
        "    print(f\"Unique classes in prediction: {np.unique(predicted_mask)}\")\n",
        "\n",
        "    # Create colored mask\n",
        "    color_map = {\n",
        "        0: [0, 0, 0], 1: [255, 0, 0], 2: [0, 255, 0], 3: [0, 0, 255],\n",
        "        4: [255, 255, 0], 5: [255, 0, 255], 6: [0, 255, 255], 7: [255, 128, 0],\n",
        "        8: [128, 0, 255], 9: [255, 192, 203], 10: [0, 128, 128], 11: [128, 128, 0],\n",
        "        12: [192, 192, 192]\n",
        "    }\n",
        "\n",
        "    colored_mask = np.zeros((predicted_mask.shape[0], predicted_mask.shape[1], 3), dtype=np.uint8)\n",
        "    for class_id, color in color_map.items():\n",
        "        colored_mask[predicted_mask == class_id] = color\n",
        "\n",
        "    print(\"‚úì Prediction complete!\")\n",
        "    return original_image, predicted_mask, colored_mask\n",
        "\n",
        "\n",
        "def visualize_prediction(original_image, predicted_mask, colored_mask, save_path=None):\n",
        "    \"\"\"Visualize the prediction results\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    axes[0].imshow(original_image, cmap='gray')\n",
        "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(colored_mask)\n",
        "    axes[1].set_title('Predicted Segmentation', fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    overlay = cv2.cvtColor(original_image, cv2.COLOR_GRAY2RGB)\n",
        "    overlay = cv2.addWeighted(overlay, 0.6, colored_mask, 0.4, 0)\n",
        "    axes[2].imshow(overlay)\n",
        "    axes[2].set_title('Overlay', fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Create legend\n",
        "    unique_classes = np.unique(predicted_mask)\n",
        "    legend_patches = []\n",
        "    color_map = {\n",
        "        0: [0, 0, 0], 1: [255, 0, 0], 2: [0, 255, 0], 3: [0, 0, 255],\n",
        "        4: [255, 255, 0], 5: [255, 0, 255], 6: [0, 255, 255], 7: [255, 128, 0],\n",
        "        8: [128, 0, 255], 9: [255, 192, 203], 10: [0, 128, 128], 11: [128, 128, 0],\n",
        "        12: [192, 192, 192]\n",
        "    }\n",
        "\n",
        "    for class_id in unique_classes:\n",
        "        if class_id in UNIFIED_CATEGORIES:\n",
        "            color = np.array(color_map[class_id]) / 255.0\n",
        "            label = UNIFIED_CATEGORIES[class_id]\n",
        "            legend_patches.append(mpatches.Patch(color=color, label=label))\n",
        "\n",
        "    plt.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"‚úì Visualization saved to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# MANUAL IMAGE UPLOAD AND PREDICTION\n",
        "# ========================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SINGLE IMAGE PREDICTION - MANUAL UPLOAD\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Model path\n",
        "MODEL_PATH = os.path.join(PROJECT_FOLDER_IN_DRIVE, 'unet_combined_best.pth')\n",
        "\n",
        "# Check if model exists\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"\\n‚ùå Model not found at: {MODEL_PATH}\")\n",
        "    print(\"Please train the model first or update the MODEL_PATH variable.\")\n",
        "else:\n",
        "    print(\"\\nüì§ Please upload an OCT image for segmentation prediction\")\n",
        "    print(\"Click 'Choose Files' button below to upload your image...\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        # Get the uploaded filename\n",
        "        uploaded_filename = list(uploaded.keys())[0]\n",
        "        print(f\"\\n‚úì Image uploaded: {uploaded_filename}\")\n",
        "\n",
        "        # Save uploaded image temporarily\n",
        "        temp_image_path = f\"/content/{uploaded_filename}\"\n",
        "\n",
        "        # Read the uploaded image\n",
        "        try:\n",
        "            # Try to read the image\n",
        "            img = Image.open(io.BytesIO(uploaded[uploaded_filename]))\n",
        "\n",
        "            # Convert to grayscale if needed and save\n",
        "            if img.mode != 'L':\n",
        "                img = img.convert('L')\n",
        "\n",
        "            img.save(temp_image_path)\n",
        "            print(f\"‚úì Image saved to: {temp_image_path}\")\n",
        "\n",
        "            # Predict\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            original, mask, colored = predict_single_image(\n",
        "                image_path=temp_image_path,\n",
        "                model_path=MODEL_PATH,\n",
        "                device=DEVICE\n",
        "            )\n",
        "\n",
        "            # Visualize\n",
        "            save_path = os.path.join(PROJECT_FOLDER_IN_DRIVE, 'prediction_result.png')\n",
        "            visualize_prediction(original, mask, colored, save_path=save_path)\n",
        "\n",
        "            # Print statistics\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(\"PREDICTION STATISTICS\")\n",
        "            print(\"=\" * 70)\n",
        "            unique, counts = np.unique(mask, return_counts=True)\n",
        "            total_pixels = mask.size\n",
        "\n",
        "            print(f\"\\n{'Class ID':<10} {'Class Name':<25} {'Pixels':<12} {'Percentage':<10}\")\n",
        "            print(\"-\" * 70)\n",
        "            for class_id, count in zip(unique, counts):\n",
        "                class_name = UNIFIED_CATEGORIES.get(class_id, 'Unknown')\n",
        "                percentage = (count / total_pixels) * 100\n",
        "                print(f\"{class_id:<10} {class_name:<25} {count:<12} {percentage:<10.2f}%\")\n",
        "\n",
        "            # Clean up temporary file\n",
        "            if os.path.exists(temp_image_path):\n",
        "                os.remove(temp_image_path)\n",
        "                print(f\"\\n‚úì Temporary file cleaned up\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error processing image: {e}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No image was uploaded. Please run this cell again to upload an image.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"To predict on another image, simply run this cell again!\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
